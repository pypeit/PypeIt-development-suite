#!/usr/bin/env python3
#
# See top-level LICENSE.rst file for Copyright information
#
# -*- coding: utf-8 -*-

"""
This script generates a kubernetes YAML file to run the PypeIt development suite 
"""

import shutil
import os
import io, yaml

from IPython import embed


def parser(options=None):
    import argparse

    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter,
                                     description='Generate a kubernetes YAML file for a Nautilus dev suite job')

    parser.add_argument('name', type=str, default=None, 
                        help='Name of the job (must consist of lower case alphanumeric characters, "-" or ".")')
    parser.add_argument('outfile', type=str, default=None, help='Name of the YAML outfile')
    parser.add_argument('--pypeit_fork', type=str, default='https://github.com/pypeit/PypeIt.git',
                        help='full clone path to pypeit fork to use')
    parser.add_argument('--dev_fork', type=str,
                        default='https://github.com/pypeit/PypeIt-development-suite.git',
                        help='full clone path to dev-suite fork to use')
    parser.add_argument('-p', '--pypeit_branch', type=str, default='develop', help='Name of the PypeIt branch to test')
    parser.add_argument('-d', '--dev_branch', type=str, default='develop', help='Name of the PypeIt Dev-Suite branch to use')
    parser.add_argument('-w', '--from_wheel', default=False, action='store_true', help='If set build a PypeIt wheel and install PypeIt from that. The default behavior is to install PypeIt directly from git.')
    parser.add_argument('--ncpu', type=int, default=8, help='Number of CPUs request')
    parser.add_argument('--ram', type=int, default=100, help='Amount of RAM to request (Gi)')
    parser.add_argument('--storage', type=int, default=400, help="Amount of storage to request (Gi). A buffer of 50 Gi is added for the limit.")
    parser.add_argument('--coverage', default=False, action="store_true", help="Collect code coverage data.")
    parser.add_argument('--container', type=str, default='python3.12', help="What docker container to use. 'pypeit' for the latest pypeit conatiner. 'python3.9', 'python3.10', etc for a specific python version. Or the full path to a different image.")
    parser.add_argument('--priority_list', default=False, action="store_true", help="Copy the test_priority_list to S3 after testing.")
    parser.add_argument('additional_args', type=str, nargs='*', default=["all"], help="Additional arguments to pypeit_test. Defaults to 'all'. "
                                                                                      "For example, if you would like to run all tests, but only reduce "
                                                                                      "the data for one instrument, you could append the following string: "
                                                                                      "'all --instruments shane_kast_blue shane_kast_red' to the end of "
                                                                                      "your command (including the quotes).")
    #parser.add_argument('-d', '--dryrun', default=False, action='store_true',
    #                    help='Only list the steps')

    return parser.parse_args() if options is None else parser.parse_args(options)


def main():

    pargs = parser()

    # Load the default
    def_yaml_file = os.path.join(os.getenv('PYPEIT_DEV'), 
                                 'nautilus', 
                                 'kube_dev_suite.yaml')
    with open(def_yaml_file, 'r') as stream:
        data = yaml.safe_load(stream)

    # Modify
    data['metadata']['name'] = pargs.name.lower()

    # Resources
    requests = data['spec']['template']['spec']['containers'][0]['resources']['requests']
    requests['cpu'] = str(pargs.ncpu)
    requests['memory'] = f'{pargs.ram}Gi'
    requests['ephemeral-storage'] = f'{pargs.storage}Gi'

    # Limits
    limits = data['spec']['template']['spec']['containers'][0]['resources']['limits']
    limits['cpu'] = str(pargs.ncpu + 1)
    limits['memory'] = f'{pargs.ram + 10}Gi'
    limits['ephemeral-storage'] = f'{pargs.storage+50}Gi'

    ###### Args #####
    arguments = pargs.additional_args
    if pargs.coverage:
        arguments += ["--coverage", "coverage.report"]

    # Install apt packages and setup PypeIt git repository
    my_args = 'apt-get -y update; apt-get -y install git awscli build-essential qtbase5-dev rclone;'
    my_args += ' cd /tmp;'
    my_args += f' git clone --branch {pargs.pypeit_branch} --depth 1 {pargs.pypeit_fork} PypeIt;'
    my_args += ' cd PypeIt;' 

    # Install PypeIt
    if pargs.from_wheel:
        # Build a PypeIt wheel and install from it
        my_args += ' pip install setuptools extension_helpers build;'
        my_args += ' python -m build --sdist --wheel;'
        my_args += ' whl_file=(dist/pypeit*.whl);'
        my_args += ' pip install ${whl_file}[dev];'
    else:
        # Install directly from git
        my_args += ' pip install -e ".[dev]";'

        # Telluric
        # These are not installed when running from a wheel so that the cache code to download
        # the telluric data is tested.
        my_args += ' cd pypeit/data/telluric/atm_grids;'
        my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/telluric/atm_grids/TelFit_MaunaKea_3100_26100_R20000.fits /tmp/telluric/TelFit_MaunaKea_3100_26100_R20000.fits --no-progress;'
        my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/telluric/atm_grids/TelFit_LasCampanas_3100_26100_R20000.fits /tmp/telluric/TelFit_LasCampanas_3100_26100_R20000.fits --no-progress;'
        my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/telluric/atm_grids/TellPCA_3000_10500_R120000.fits /tmp/telluric/TellPCA_3000_10500_R120000.fits --no-progress;'
        my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/telluric/atm_grids/TellPCA_3000_26000_R10000.fits /tmp/telluric/TellPCA_3000_26000_R10000.fits --no-progress;'
        my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/telluric/atm_grids/TellPCA_3000_26000_R15000.fits /tmp/telluric/TellPCA_3000_26000_R15000.fits --no-progress;'
        my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/telluric/atm_grids/TellPCA_3000_26000_R25000.fits /tmp/telluric/TellPCA_3000_26000_R25000.fits --no-progress;'
        my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/telluric/atm_grids/TellPCA_9300_55100_R60000.fits /tmp/telluric/TellPCA_9300_55100_R60000.fits --no-progress;'
        my_args += ' ln -s /tmp/telluric/* .;'
    
    # Dev suite
    my_args += ' cd /tmp;'
    my_args += f' git clone --branch {pargs.dev_branch}  --depth 1 {pargs.dev_fork} PypeIt-development-suite;'
    my_args += ' cd PypeIt-development-suite;'
    my_args += ' source source_headless_test.sh;'
    # Pixel flat
    my_args += ' echo Copying CALIBS from Google Drive...; rclone --config nautilus/rclone.conf copy gdrive:CALIBS/ CALIBS/;'
    # Raw Data
    my_args += ' echo Copying RAW_DATA from Google Drive...; rclone --config nautilus/rclone.conf copy gdrive:RAW_DATA/ RAW_DATA/;'
    # Run the test 
    my_args += f' ./pypeit_test -t {pargs.ncpu} {" ".join(arguments)} -r pypeit.report -o /tmp/REDUX_OUT --csv performance.csv;'

    # Tar up the logs
    my_args += f' find /tmp/REDUX_OUT -name "*test*log" -print | tar cfz logtar.tar.gz -T -;'

    #Copy results back to s3    
    my_args += f' aws --endpoint $ENDPOINT_URL s3 cp pypeit.report s3://pypeit/Reports/{pargs.name}.report;'
    my_args += f' aws --endpoint $ENDPOINT_URL s3 cp performance.csv s3://pypeit/Reports/{pargs.name}_performance.csv;'
    my_args += f' aws --endpoint $ENDPOINT_URL s3 cp logtar.tar.gz s3://pypeit/Logs/{pargs.name}.logs.tgz;'
    if pargs.coverage:
        my_args += f' aws --endpoint $ENDPOINT_URL s3 cp coverage.report s3://pypeit/Reports/{pargs.name}.coverage.report;'
    if pargs.priority_list:
        my_args += f' aws --endpoint $ENDPOINT_URL s3 cp test_priority_list s3://pypeit/Reports/{pargs.name}.test_priority_list;'

    data['spec']['template']['spec']['containers'][0]['args'][0] = my_args

    if pargs.container.startswith("python"):
        python_version = pargs.container[6:]
        data['spec']['template']['spec']['containers'][0]['image'] = f"docker.io/library/python:{python_version}"
    else:
        data['spec']['template']['spec']['containers'][0]['image'] = pargs.container

    with io.open(pargs.outfile, 'w', encoding='utf8') as outfile:
        yaml.dump(data, outfile, default_flow_style=False, allow_unicode=True)

    # Help
    print("\n\n=======================================")
    print("Helpful Hints:")
    print("=======================================")
    print(f"\n1) Launch the job with: \n\n kubectl -n pypeit create -f {pargs.outfile} \n")
    print(   "2) Monitor by going here: \n\n https://grafana.nrp-nautilus.io/d/85a562078cdf77779eaa1add43ccec1e/kubernetes-compute-resources-namespace-pods?orgId=1&refresh=10s&var-datasource=default&var-cluster=&var-namespace=pypeit \n")

if __name__ == '__main__':
    # Giddy up
    main()
